<!DOCTYPE html>
<html>
<head>
  <meta http-equiv="content-type" content="text/html; charset=utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>[DS]5회차  | 데이터 사이언스 블로그</title>
  <meta name="description" content="대충 넘어 갔던 개념을 자세하게 따져보는 블로그 '[DS]5회차'을 한 번 살펴보세요.">
  <meta property="og:title" content="[DS]5회차">
  
  <meta property="og:type" content="article">
  <meta property="article:published_time" content="2021-03-18">
  
  <meta property="og:description" content="대충 넘어 갔던 개념을 자세하게 따져보는 블로그 '[DS]5회차'을 한 번 살펴보세요.">
  <meta property="og:url" content="https://uqpuark.github.io/dsstudy/5_neural_network/">
  <meta property="og:site_name" content="데이터 사이언스 블로그">
  
  <meta property="og:image" content="https://uqpuark.github.io/images/thumbnail.png">
  
  
  <link rel="icon" href="/favicon.ico" type="image/x-icon">
  <link rel="canonical" href="https://uqpuark.github.io/dsstudy/5_neural_network/">
  <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
  <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.6/styles/agate.min.css">
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto+Sans+KR&display=swap">
  <link rel="stylesheet" href="/css/styles.css">
  <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.6/highlight.min.js"></script>
  <script>hljs.initHighlightingOnLoad();</script>
  
  
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-189694109-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());
    gtag('config', 'UA-189694109-1');
  </script>
  
  
  <script type="text/javascript">
  function toggle_visibility(id) {
    var e = document.getElementById(id);
    if (e.className === 'menu')
      e.className = 'menu hidden';
    else
      e.className = 'menu';
  }
  </script>
</head>
<body>
  <div class="navbar">    
    <div class="logo">
      <a href="/">
        <img src="/images/logo.png" height="35px" />
      </a>
    </div>
    <div class="burger">
      <button onclick="toggle_visibility('menu')">
        <i class="fa fa-bars" aria-hidden="true"></i> 메뉴
      </button>
    </div>
    <div id="menu" class="menu hidden">
      <ul>
        <li><form id="search"
    action='' method="get">
    <label hidden for="search-input">Search site</label>
    <input type="text" id="search-input" name="query"
    placeholder="search or jump to...">
    <input type="submit" value="search">
</form>

</li>
        <li><a href="/categories">카테고리</a></li>
        <li><a href="/tags">태그</a></li>


      </ul>
      <input class="search" id="search-input" type="search" placeholder="검색어" value="">
    </div>
  </div>
  <div class="container">    


<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css" integrity="sha384-AfEj0r4/OFrOo5t7NnNe46zW/tFgW6x/bCJG8FqQCEo3+Aro6EYUG4+cU+KJWu/X" crossorigin="anonymous">
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.js" integrity="sha384-g7c+Jr9ZivxKLnZTDUhnkOnsh30B4H0rpLUpJ4jAIKs4fnJI+sEnkvrMWph2EDg4" crossorigin="anonymous"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/contrib/auto-render.min.js" integrity="sha384-mll67QQFJfxn0IYznZYonOWZ644AWYC+Pt2cHqMaRhXVrursRwvLnLaebdGIlYNa" crossorigin="anonymous"
    onload="renderMathInElement(document.body);"></script>


<div class="post">
  <div class="post-title">
    <a href="https://uqpuark.github.io/dsstudy/5_neural_network/">
      <div class="post-meta">
        <time>2021년 03월 18일 21시 29분</time>
        <h1>[DS]5회차</h1>
      </div>
    </a>
  </div>
  <section class="post-content">
    <h1 id="스터디를-위해-잠시-오픈-일요일-오후-2시">스터디를 위해 잠시 오픈 (일요일 오후 2시)</h1>
<h1 id="5회차----neural-network-basic">5회차  - Neural Network (Basic)</h1>
<p>4회차 리뷰</p>
<ul>
<li>2rd moment에 적용하는 게 PCA</li>
<li>PCA는 \( x_i \)들의 linear combination으로 구성</li>
<li>FA와 PCA는 직관적으로 해석시 굉장히 유사성이있다.</li>
</ul>
<p>시작
뉴럴 네트워크는 지금까지 배운 통계학 모델들의 결합에 불과하다.</p>
<h1 id="section-1-신경망-모델-neural-networks">Section 1. 신경망 모델 (Neural Networks)</h1>
<h2 id="neural-network-모델의-아이디어">Neural network 모델의 아이디어</h2>
<h3 id="deep-learning--인공지능">Deep Learning = 인공지능?</h3>
<h4 id="인간의-두뇌와-컴퓨터-시스템-비교">인간의 두뇌와 컴퓨터 시스템 비교</h4>
<p>뉴런 : 전기 신호가 발생하고, 신경전달물질이 시냅스 틈으로 방출된다. 시냅스는 다음 뉴런으로 신경전달물질을 보내는 구조.</p>
<h4 id="neural-network-a-poor-copy-of-neuron-network">Neural network: A (poor) copy of neuron network</h4>
<h3 id="딥러닝은-좀-더-무거운-패턴-인식-알고리즘일-뿐이다">딥러닝은 (좀 더 무거운) 패턴 인식 알고리즘일 뿐이다.</h3>
<h4 id="딥러닝-시스템과-인간-두뇌의-작동-방식">딥러닝 시스템과 인간 두뇌의 작동 방식</h4>
<ul>
<li>뉴럴네트워크에 뉴런의 구조를 동일하게 구현
<ul>
<li>정보가 입력되고 처리한 다음에 결과값을 방출하는 시스템</li>
</ul>
</li>
</ul>
<h4 id="r의-neuralnet-패키지로-돌린-neural-network딥러닝-모델">R(의 neuralnet 패키지)로 돌린 Neural Network(딥러닝) 모델</h4>
<figure>
    <img src="/images/pabii/neuralnetwork00.jpg"/> 
</figure>

<ul>
<li>변수 8개를 넣었는데 왜 9개의 노드가 나오는가? : Brand라는 변수를 (삼성, LG, Others)로 인식했기 때문이다</li>
<li>bias는 사실상 y절편
<ul>
<li>dummy variable 처리가 필수(dummification하면 y절편이 없어져야 된다.)</li>
<li>y절편(bias) 필수 : 하나의 벡터값을 빼줌으로 Linear dependency가 붕괴되어 linearly independence를 회복한다
<ul>
<li>더미피케이션 했으면 하나의 값을 빼는 방식으로 계산하지 않으면 랭크 프로블럼이 생겨서 계산이 안될 것이다.</li>
</ul>
</li>
<li>모든 NN 컨셉은 Regression</li>
</ul>
</li>
<li>노드가 (4,2)인 이유를 제대로 설명하지 못한다.</li>
</ul>
<h2 id="neural-network-모델-간단-예제">Neural network 모델 간단 예제</h2>
<h3 id="왜-multi-layer-모델이-필요할까-왜-multi-layer-모델이-더-적합도를-높여줄-수-있을까">왜 Multi-layer 모델이 필요할까? 왜 Multi-layer 모델이 더 적합도를 높여줄 수 있을까?</h3>
<h4 id="hidden-layer가-없는-모델-mcculloh-pitts-1943년">Hidden layer가 없는 모델 (McCulloh-Pitts, 1943년)</h4>
<figure>
    <img src="/images/pabii/neuralnetwork01.jpg"/> 
</figure>

<ul>
<li>X -&gt; Y의 단순 구조에서 벗어나 X -&gt; Y -&gt; Z와 같은 형태로 응용이 가능하다, Z의 형태에 따라 기존 모델 보다 더 나은 예측치를 보일 수 있음</li>
<li>위의 그래프 모델을 functional shape으로 바꾸면</li>
</ul>
<p>$$ w_0 + w_1 \cdot x_1 + w_2 \cdot x_2 = y $$</p>
<p>\( w_0 \)가 일종의 앵커 역할, 높낮이 역할</p>
<p>ㅡㅡㅡㅡㅡ</p>
<ul>
<li>위의 그림은 linear regression을 그래프(네트워크)형태로 나타낸 것일 뿐</li>
<li>뉴럴 네트워크 모델도 다 bias항(y절편)을 가지고 있다.</li>
<li>퍼셉트론 모델</li>
</ul>
<blockquote>
<p>기본 모형을 좀 더 발전시켜보자.<br>
\( x, z, Y \)가 있다고 보자. 레이어를 하나 더 추가하면 어떨까?<br>
왜 이러한 작업을 하는 것일까?<br>
\(z\)는 \( x_1, x_2\)의 결합으로 나타나는 건데, 무슨 효과가 있을까?</p>
</blockquote>
<h4 id="multi-layer-좌측-모델에서-z가-필요한-이유">Multi-layer (좌측 모델에서 Z)가 필요한 이유</h4>
<figure>
    <img src="/images/pabii/neuralnetwork02.jpg"/> 
</figure>

<ul>
<li>
<p>XOR 문제에서 직선 하나만으로 classification을 할 수 없다. 최소한 2개 이상의 선이 필요하다.<br>
2개의 직선으로 구분하려고 하는데, 제일 쉬운게 x축, y축에 수직인 선 2개(상수함수) -&gt; Decision Tree<br>
그렇다면 수직선(0차)말고 직선(1차)으로 곡선(2차식 이상의 방정식)과 같은 효과를 볼수 있을 까? -&gt; NN</p>
</li>
<li>
<p>식 2개를 써서 끝나는게 아니라<br>
값을 재배정해줘서(매핑을 새로해주는 것) 0과 1로 classify가 가능할 수 있게 모델링 하겠다</p>
</li>
<li>
<p>(?) 첫번째 식은 새로운 축을 만들어 주는 개념. 예) 축 밖이면 0 안이면 1</p>
</li>
</ul>
<p>ㅡㅡㅡㅡㅡ</p>
<ul>
<li>선 하나를 사용하는 경우 : 직선이 아니라 곡선으로 구분해야 -&gt; 로지스틱 리그레션, SVM</li>
<li>곡선 어려우니깐 직선으로 하자 : 가장 단순한 직선 : DT에서 사용하는 좌표축에 수직인 선(상수함수)</li>
<li>그런데, 뉴럴 네트워크는 DT가 아니니깐 다른 아이디어를 사용 : 조금 더 복잡한 식을 긋는 아이디어(1차식에 해당하는 대각선), 대각선을 그리면 수직선보다 보다 완벽하게 구분을 할 수 있다.
<ul>
<li>뉴럴 네트워크가 완벽한 모델이 아니라 DT를 약간 바꾼 모델이다.</li>
<li>x=3이 아니라 x+y=2인 식을 사용하는 것</li>
<li>뉴럴 네트워크 모델은 DT를 리니어 리그레션 또는 로지스틱 리그레션 형태의 결합으로 업그레이드 한 것.</li>
<li>2개의 Decision boundary를 결합함으로서 문제를 풀기 때문에 결국 로지스틱 리그레션이라고 이해를 할 수가 있다.(?)</li>
</ul>
</li>
</ul>
<p>한 줄 요약 : NN = DT + LR</p>
<h3 id="feed-forward--back-propagation-계산-방식---1">Feed Forward + Back propagation 계산 방식 - 1</h3>
<h4 id="모델-예제--계산-방식">모델 예제 + 계산 방식</h4>
<figure>
    <img src="/images/pabii/neuralnetwork03.jpg"/> 
</figure>

<ul>
<li>
<p>왜 식을 굳이 2번 써서 모델을 복잡하게 만들까? (X -&gt; Z -&gt; Y)</p>
<ul>
<li>3차원에서 시작해서 2차원으로 매핑하고, 매핑된 상태에서 0과 1을 분류한다.</li>
</ul>
</li>
<li>
<p>PC와 매우 비슷하다.</p>
<ul>
<li>공통점 : loading 계산 방식과 유사</li>
<li>차이점 : 단 2개의 변수를 이용하여 강제로 매핑(FA와 비슷하다), Var과 Cov 둘다 따지는 개념</li>
</ul>
</li>
<li>
<p>부스팅 방식으로 w를 계속 업데이트</p>
<ul>
<li>feed forward + back propagation</li>
</ul>
</li>
</ul>
<p>뉴럴 네트워크는 각 모델들의 장점 뿐만 아니라 단점도 다 가져왔다. 따라서 단점을 커버하는 방식도 똑같이 해줘야 한다.</p>
<p>계산이 다 행렬을 계산하는 일이기 때문에 computational cost가 많이든다. GPU연산에 관심을 두기 시작</p>
<blockquote>
<p>feed forward<br>
back propagation</p>
</blockquote>
<p>ㅡㅡㅡ</p>
<ol start="0">
<li>무조건 Normalize(=feature scaling)을 해줘야 한다 : A값이 큰 영향을 주고, B값이 작은 영향을 주면서 전체 모델의 convergence에 영향을 줄 수 있다.
<ul>
<li>try and error 계산을 할 때는 무조건 scaling을 해줘서 오차가 날 수 있는 확률을 최소화 해야한다.</li>
</ul>
</li>
<li>가중치(weight)와 바이어스(bias)를 임의로 지정한다.
<ul>
<li>새로 데이터가 입력이 되면 계산이 진행되겠죠. (random point에서 시작)</li>
<li>계속 값을 고쳐주는(업데이트) 작업 : Back propagation</li>
</ul>
</li>
<li>NN의 계산 방식 : feed forward + back propagation의 결합 (w값을 dynamic updating)</li>
<li><strong>부스팅</strong> 계열의 모델 아이디어
트레이닝 데이터가 주어지면 각각 그룹을 10개 정도 나누고, 각각의 그룹을 입력하고, 입력값으로 w를 업데이트 합니다.
<ul>
<li>2번째 데이터를 입력할 때 이미 업데이트 값을 사용하면 또 다시 새로운 데이터에서 업데이트<br>
이 예제에서는 10번의 업데이팅 작업이 있을 것이다.</li>
</ul>
</li>
</ol>
<p>결론 : NN모델은 <strong>DT</strong>를 가져다가 <strong>LR</strong>형태로 쓰는데 parameter 업데이팅 방식이 <strong>Boosting</strong> 방식이다.</p>
<ul>
<li>우리가 배운 모델들의 결합 형태</li>
</ul>
<blockquote>
<p>빅데이터(예: 10억개)에서 뉴럴네트워크 계산이 빠르게 될까?</p>
<ul>
<li>절대 안됨, 엄청 느려진다.</li>
<li>XGboosting에서 stochastic의 개념을 사용한것처럼</li>
<li>뉴럴 네트워크에서는 다양한 종류의 하드웨어(GPU) 자원을 쓰는 방식으로 속도를 올린다.
<ul>
<li>행렬 계산을 GPU를 이용해서 연산(예 : 비트코인 채굴)</li>
</ul>
</li>
</ul>
</blockquote>
<h3 id="feed-forward--back-propagation-계산-방식---2">Feed Forward + Back propagation 계산 방식 - 2</h3>
<h4 id="모델-내-z값-및-y값-계산--계산-방식">모델 내 Z값 및 Y값 계산 + 계산 방식</h4>
<p>예제에서 최종 Y에 해당하는 값이 sigmoid function사용 -&gt; 확률 비슷한거라고 생각해보자(overconfidence 없다고 생각).<br>
분산=\( np(1-p) \), 실제 y값이 1인 경우 : \( \hat{Y}\) 0.475를 오차항의 결합부분만큼 보정해주는데 사용. Z의 값을 보정하고, W를 다시 업데이트 해주는 과정의 반복</p>
<h3 id="기준점-threshold가-여러개인-경우">기준점 (Threshold)가 여러개인 경우</h3>
<h4 id="그래프-예시">그래프 예시</h4>
<ul>
<li>
<p>뉴럴 네트워크는 DT와 같은 방식(대신 축에 수직선이 아닌 직선)으로 데이터를 구분</p>
<ul>
<li>Tree의 단점도 같이 가져옴 : layer와 feature가 많아지면 pruning이 어려움</li>
</ul>
</li>
<li>
<p>노드를 계속 추가하면?(fat)</p>
<ul>
<li>DT에서 하는 것처럼 선들이 많이 추가되면 로지스틱 리그레션과 같은 커브를 구현할 수 있다.(직선으로 곡선을 표현)</li>
<li><a href="https://youtu.be/3d6DsjIBzJ4?t=98">Talyor&rsquo;s expansion</a> : 차수가 추가될수록 곡선(원래 함수에 근사)</li>
</ul>
</li>
<li>
<p>레이어를 계속 추가하면?(long)</p>
<ul>
<li>예) 3차원에서 다시 2차원으로 매핑</li>
<li>현재 스페이스에서 매핑을 새롭게 시도, 구획을 더 나눠주는 시도</li>
<li>topology 관점(?)</li>
</ul>
</li>
<li>
<p>예) 사람 얼굴 인식</p>
<ul>
<li>눈, 코, 입 구획을 나눠보고 싶다.</li>
<li>턱선을 계산하고 싶다.</li>
</ul>
</li>
</ul>
<figure>
    <img src="/images/pabii/neuralnetwork04.jpg"/> 
</figure>

<p>Hidden layer의 노드 2개(초록, 주황)을 일종의 factor(latent variable)이라고 생각해 볼 수 있다.<br>
N차원에서 2차원으로</p>
<ul>
<li>첫번째 스텝에서는 linear 함수 사용</li>
<li>두번째 스텝에서 activation function이 non-linear</li>
<li>따라서 non-linear의 관점으로 볼 수 있다.</li>
</ul>
<p>FA의 관점에서 바라보면 우주에서 지구와 화성만 중요하니까 2개의 노드만 남긴 것.
Topology 관점으로 볼 때 점점 구획을 나눠보는 개념</p>
<blockquote>
<p>몇 개의 노드(팩터)가 좋은지 인간이 정해주는 것인데, 몇개로 정해야되는지 잘 모르기 때문에 PCA로 출발점, 기준을 잡는 것이다.</p>
</blockquote>
<p>ㅡㅡㅡ</p>
<p>빨간생 영역을 찾을 수 있게 된다.</p>
<ul>
<li>2차원 평면 위</li>
<li>입력값은 2개(x,y)</li>
<li>hidden layer를 하나 거쳐감으로 해서 찾아낼 수 있다.</li>
</ul>
<p>입력값을 하나 더 넣었다면?  : 3차원 평면이 되었을 것<br>
히든 레이어에 z가 하나 더 넣었으면? : 그래프가 하나 더 나오고, 새로운 영역에서 구분하게 된다.<br>
새로운 2차 레이어 하나를 더 넣었으면? : 특정한 공간을 찾아내는 작업을 한번 더 하는것</p>
<ul>
<li>1차 레이어에서 구분된 공간을 새로운 2차원 평면이라고 하고, 그 평면에서 다시 새로운 영역을 찾아내는 작업(공간의 재구성)</li>
</ul>
<p>레이어 추가 : 점점 공간을 축소해나가는데, 집중해 있는 공간을 나의 새 공간이라고 인지하는 작업<br>
뉴런을 추가 : 새로운 직선을 점점점 추가한다는 이야기, 직선이 모여서 곡선이 된다. linear를 모아 non-linear를 찾는 작업</p>
<ul>
<li>히든 레이어 어떻게 구성해야합니까? : 정해진 규칙 없음, non-parametric model이기 때문, but 기본적인 특징은 이해해야됨</li>
</ul>
<p>레이어의 depth를 깊게 하는 것 : 공간을 점점 더 집중해서 보는 관점<br>
하나의 레이어 안에 여러개의 뉴런을 추가 : linear function을 조금 더 non-linear하게 만들어 주는 작업의 일환이다라는 관점</p>
<ul>
<li>예) 전체 우주가 있는데 우리은하랑 안드로메다은하가 있는데 우리은하에서 태양계 찾고 안드로에서 태양계 비슷한거 찾고, 지구 찾고, 비슷한거 찾고하면서 점점 공간을 축소해나가는데, 새로운 공간 완전히 하나의 점을 집중한 다음에 집중해 있는 공간을 나의 새 공간이라고 인지하는 작업</li>
</ul>
<p>이렇게 이해하고 나면</p>
<ul>
<li>기존 : 뉴럴네트워크에서 히든레이어를 (5,3) 등등 어떤게 좋은지 몰라서 그냥 바꿔가면서 작업해 볼게요 하다가</li>
<li>이해 후 : 제 데이터의 특성이 공간을 계속 집중해서 찾아가야 되는 예를 들어 이미지인식이어서 얼굴에서 눈을 찾고, 눈에서 눈알, 눈알 위에 눈썹을 찾는 관점이다. 그러니깐 이렇게 접근해야 할 것 같아요라고 할 수 있고,<br>
제 데이터는 복잡한 논리니어 펑션인데, 이 펑션 찾기가 힘들어서 이 방법을 빌려왔기 때문에 뎁스를 길게하기 보다는 한 레이어에 많은 변수를 집어넣는 방식으로 처리해야될것 같습니다. 라고 생각을 할 수 있겠죠</li>
</ul>
<p>이런 생각의 전환만 일어난다고 해도 저는 이 수업 들은 값을 한다고 생각합니다.<br>
문제는 computation cost를 굉장히 많이 지불해야하고, overfitting의 위험에 노출되어있다.<br>
히든 레이어 숫자, 인풋의 숫자, 아웃풋의 숫자, 각 레이어 마다 인풋이 얼마나 늘어나는지에 따라 모델의 계산이 훨씬 더 많아 진다는 것을 인지를 해야한다.<br>
트리가 복잡해질 수록 프루닝이 복잡해지겠죠.(왜 프루닝이냐고? NN도 트리모델이니깐)<br>
Graph Theory 수업에서 트리모델, 네트워크모델 배우고, NN은 네트워크 모델에 일환에 불과합니다. 당연히 모든 트리모델은 프루닝 작업을 이용해서 오버피팅 문제를 해결하기 때문에 프루닝이라 불러요.</p>
<h2 id="network의-형태---1">Network의 형태 - 1</h2>
<h3 id="네트워크가-구성되는-방식에-따라">네트워크가 구성되는 방식에 따라</h3>
<figure>
    <img src="/images/pabii/neuralnetwork05.jpg"/> 
</figure>

<p>지금까지 본 대부분의 네트워크 모델들은 Feed-forward : 데이터가 입력 된 후, 가공하고, 결과값을 쏘는<br>
뉴럴 네트워크 모델은 back propagation(후치환) 추가<br>
RNN 모델(Recurrent) : feed forward 모델과 거의 동일한데, 기억을 하고 있다가 그 기억을 다시 되살려주는 작업이 포함되어있다.<br>
Complete : 모든 컴포넌트들이 서로 연결이 되어있는 구조 : 예) 블록체인</p>
<h2 id="network-theory-기본-개념">Network Theory 기본 개념</h2>
<h3 id="네트워크-모델은-tree-모델의-사우이-개념---metric--network-distance-euclidean-distance-x">네트워크 모델은 Tree 모델의 사우이 개념 - Metric = Network distance (Euclidean distance X)</h3>
<h4 id="network-model-basic">Network Model basic</h4>
<ul>
<li>Vertices(Node)</li>
<li>Edges(arcs)</li>
</ul>
<p>Metcalfe&rsquo;s Law : 네트워크의 가치는 사용사 수의 제곱에 비례한다</p>
<h4 id="network--model-예시">Network  Model 예시</h4>
<figure>
    <img src="/images/pabii/neuralnetwork06.jpg"/> 
</figure>

<ul>
<li>페이스북이 처음에 Mesh 형태로 네트워크 모델을 만들다가 결국 Extended Star에 마름모를 그려서 모델을 만들었다.
<ul>
<li>Mesh 모델에서 노드가 무한대가 되면 스케일링이 불가능해지기 때문</li>
</ul>
</li>
</ul>
<h2 id="network-theory---직접-계산">Network Theory - 직접 계산</h2>
<h3 id="matrix-형태로-변환-후-계산에-활용">Matrix 형태로 변환 후 계산에 활용</h3>
<h4 id="network---matrix-계산-방식">Network -&gt; Matrix 계산 방식</h4>
<h4 id="network---centrality">Network -&gt; centrality</h4>
<h2 id="stability-vs-efficiency">Stability vs. Efficiency</h2>
<h3 id="중앙집권적인-시스템은-중앙이-무너지면-끝--탈중앙집권적이면-효율성이-보장될까">중앙집권적인 시스템은 중앙이 무너지면 끝 &amp; 탈중앙집권적이면 효율성이 보장될까?</h3>
<h4 id="centralized---unstable">Centralized - Unstable</h4>
<h4 id="decentralized---inefficient">Decentralized - Inefficient</h4>
<h2 id="network의-형태---2">Network의 형태 - 2</h2>
<h3 id="필요한-deepness에-따라">필요한 &ldquo;Deep&quot;ness에 따라</h3>
<p>히든레이어 하나 vs 두개의 차이</p>
<h4 id="hidden-layer를-하나만-쓰는-경우">Hidden layer를 하나만 쓰는 경우</h4>
<h4 id="hidden-layer를-여러개-쓰는-경우">Hidden layer를 여러개 쓰는 경우</h4>
<p>데이터의 구조가 닫힌형태일때, 오차값이 나오고 보정하는 작업을 할 때,<br>
고쳐지는 정보들이 이미 2번째 레이어에서 반영이 되고나서 1번째 레이어에 반영을 하기 땜누에 1번째 레이어에서 고쳐지는 부분이 적어진다. 마찬가지로 첫번째 weight도 훨씬 더 덜 업데이트 된다.<br>
즉, 보정이 되지 않는 부분들이 굉장히 많이 생길 것이다.</p>
<ul>
<li>레이어가 많아지면 안그래도 오버피팅의 문제가 있는데,</li>
<li>계속 레이어를 추가해서 퍼포먼스가 좋아지는 것을 볼수있다가(optimization), 다시 안좋아지고 (genralization), computational cost도 않좋아지는 문제가 발생한다.</li>
</ul>
<h2 id="network의-형태---3">Network의 형태 - 3</h2>
<p>Y값의 범주가 2개거나 그 이상일 수 있다. 단순히 output의 노드의 개수가 많아지는 것</p>
<h3 id="원하는-결과값의-형태에-따라">원하는 결과값의 형태에 따라</h3>
<p>아웃풋 레이어의 개수의 차이</p>
<h4 id="답이-y01로만-나올-때">답이 Y=0/1로만 나올 때</h4>
<h3 id="답이--y_1y_2y_3-으로-나올-때">답이 \( Y_1,Y_2,Y_3 \)으로 나올 때</h3>
<p>여러가지 형태의 네트워크 모델을 만들 수 있다. 보통 모델은 내가 어떻게 만드느냐에 따라서 하나의 모델로 만들어지는 것이다. linear regression 보다 more flexibility한데, 그 만큼 책임질 부분이 많아진다.</p>
<h2 id="neural-network는-결국-logistic-regression을-여러-단계로-결합한-모델이다">Neural Network는 결국 Logistic regression을 여러 단계로 결합한 모델이다</h2>
<h3 id="activation-function에-따라-logistic이-아닌-다른-regression의-결합이-될-수도-있음">Activation function에 따라 Logistic이 아닌 다른 regression의 결합이 될 수도 있음</h3>
<figure>
    <img src="/images/pabii/neuralnetwork08.jpg"/> 
</figure>

<p>Neural Network는 Logistic regression을 여러 단계로 결합했는데, 결합 방식이 Decision Tree 이고, 계산 방식이 Boosting 계열 방식이었다.</p>
<h2 id="신경망-모델을-정의하는-또-하나의-함수---activation-function">신경망 모델을 정의하는 또 하나의 함수 - Activation function</h2>
<h3 id="activation-function은-따로-정해진-것이-아니라-목적에-맞게-바꾸어가며-이용할-수-있음">Activation function은 따로 정해진 것이 아니라, 목적에 맞게 바꾸어가며 이용할 수 있음</h3>
<h4 id="모델의-결과를-크게-좌웋라는-activation-function">모델의 결과를 크게 좌웋라는 Activation function</h4>
<ul>
<li>DT의 general format이라 불렀다.</li>
<li>x값을 feature scaling한 \( x_1, x_2 \)를 집어 넣는데, 한번더 \( z_1, z_2 \)가 feature scaling된 형태가 되기를 원하는것
<ul>
<li>[0,1], [-1,1]이 되는 함수를 사용</li>
<li>non-linear approximate 해주는 것</li>
</ul>
</li>
<li>또 하나의 non-linear function을 찾는 것에 불과하다.<br>
Activation function은 실제로 하고싶은대로 집어 넣어도 상관없다. 왜냐하면 수학적인 것을 원하는 게 아니라 결과만 좋으면 되기 때문이다.<br>
다양한 종류의 Activation function, kernel 들이 존재한다.
<ul>
<li>Rectifier : 0이상의 값을 다 가져가는 함수, 이미지 인식할때 critical하게 좋다</li>
<li>Softmax : 전체 값에서 이 값이 나올 확률, 반드시 합이 1이되고, n차원으로 매핑시켜준다.</li>
</ul>
</li>
<li>vanishing gradient problem :</li>
<li>Exploding gradient problem : 모델이 수렴을 안하는 경우
<ul>
<li>해결하는 방법이 데이터 전처리(feature scaling)</li>
</ul>
</li>
</ul>
<p>ㅡㅡㅡ</p>
<p>sigmoid, hyper-tangent, ReLU(Rectified Linear Unit)</p>
<blockquote>
<p>ReLU6 : 최대값을 6으로 fixed 해놓은 함수</p>
<ul>
<li>렉티파이어 함수는 1이상으로 계속 함수값이 증가한다. 필요한 양수값은 취하고, 음수값은 전부다 버리는 방식
<ul>
<li>이미지 인식에 혁명을 가져왔다. 흑백으로 구분했을 때, 흰색은 +1, 흑색은 -1이라하면 흰색부분 만 살린다.</li>
<li>큰 값들을 모두다 가져가 버리면 scaling이 안되는 문제가 생겨서 6이상의 큰값들은 포기를 하겠다.</li>
<li>그 밖에, 한계값이 주어지는 sigmoid나 hyper-tangent의 구조를 일부 가져오는 함수도 활용한다</li>
</ul>
</li>
</ul>
</blockquote>
<h4 id="신경망-모델의-단점">신경망 모델의 단점</h4>
<p>기존의 NN은 많이 알려지지 않았고, 그 당시에는 바보들만 하는 거였다. 요즘들어서 computing power가 엄청나게 증가해서 사람들이 혹하고 달려들고 있다. 하지만 데이터 전처리를 안하면 의미가 없다.</p>
<p>ㅡㅡㅡ</p>
<p>2가지 이슈가 있다.</p>
<ol>
<li>convergence가 없는 경우
<ul>
<li>계속 모델 패러미터가 업데이트 되는 경우</li>
<li>ReLU함수 사용해서 데이터가 폭증하는 경우(Exploding gradient problem)</li>
</ul>
</li>
<li>데이터가 보정에 실패해서 마지막(첫번째 weight)까지 오면 아예 업데이트가 안되는 문제가 생기는 경우(Vanishing gradient problem)
<ul>
<li>프루닝 작업을 해줘야 하는데, 프루닝은 모델이 복잡할 수록 힘들어지는 작업이라&hellip;</li>
<li>NN을 사용한다는 의도 자체가 모델이 단순하지 않아서 logistic하지 않고 복잡한 계산을 예상하고 사용하는 것인데, 사용하다가 vanishing gradient problem을 만나면 굉장히 곤란한 상황이 생기는 겁니다</li>
</ul>
</li>
</ol>
<h5 id="정리">정리</h5>
<p>신경망 모델이 굉장히 좋은 모델이에요. 여러가지 측면에서 배운 모델들을 다 결합해서 찾아내기 힘든 non-linear을 찾아줍니다.<br>
반면, 모델이 굉장히 약합니다.</p>
<ul>
<li>데이터 구조에 따라서 모델 계산이 잘못될수도 있고,</li>
<li>모델을 복잡하게 만들수록 vanishing problem이 생기고,</li>
<li>어떤 activation function을 사용하는가 / scaling을 했는가? 에 따라서 exploding problem이 생기고</li>
<li>try and error를 많이 해야돼서 computational cost도 높다.</li>
</ul>
<p>모델의 결함을 인지한 상태에서(?) 많이 쓰이고 있다.<br>
CNN, RNN : 이러한 결함을 보완해서 사용하는 작업이 있다.</p>
<h1 id="section-2-신경망-모델의-응용">Section 2. 신경망 모델의 응용</h1>
<h2 id="deep-network-vs-fat-network">Deep Network vs. Fat Network</h2>
<h3 id="어떤-모델이-더-좋은-모델일까">어떤 모델이 더 좋은 모델일까?</h3>
<h4 id="universality-theorem">Universality Theorem</h4>
<ul>
<li>그래프가 떨어지는 이유 : vanishing gradient problem이 생기기때문
<ul>
<li>지구까지 오고 끝내야되는데, 더 들어가서 한반도, 서울, &hellip;, 집 까지 오면 너무 극단적이다.</li>
</ul>
</li>
</ul>
<p>ㅡㅡㅡ</p>
<p>Universality Theorem : 레이어에 여러개의 뉴런을 추가한 이야기와 관련이 있다.</p>
<ul>
<li>여러개 뉴런을 추가하면? Non-linearity를 만들어 낸다. -&gt; 테일러 급수(Network version)이다.
<ul>
<li>고차항을 계속 추가하면 원래 함수와 근사한 다항식을 찾아낼 수 있다. 오차는 n+1차항으로 설정.</li>
<li>마찬가지로 뉴런들을 계속 추가하면 그래프를 굉장히 복잡합 non-linear function에 피팅을 할 수 있다.(너무 많이 추가하면 cost가 많이 생기니 적절하게 타협을 봐서)</li>
</ul>
</li>
<li>이렇게 특정한 함수의 근사식을 만들어 낼 수 있다.</li>
</ul>
<p>Universality Theorem : Taylor&rsquo;s Expansion에서 다항식을 이용해서 했던 작업을 네트워크에서 트리형태로 해결해낼 수 있다는 이론</p>
<p>그래서 뉴럴 네트워크 모델은 non-linearity를 찾아내는 작업의 일환이다라고 이야기를 한다.</p>
<h5 id="multi-layer-model이-왜-더-좋은-성능을-보이는-경우가-많을까">multi-layer model이 왜 더 좋은 성능을 보이는 경우가 많을까?</h5>
<p>멀티레이어 모델 : 데이터의 숨겨져 있는 구조를 찾아내는 작업이다. 이미지 인식 같은 경우에는</p>
<ul>
<li>멀티레이어 구조가 좋을 때도 있고
<ul>
<li>예) 눈코입이 두드러지게 구분되있는 경우, 데이터 구분구분 마다 특정한 스트럭쳐를 갖고있으면 레이어 증가시 퍼포먼스 향상이 가능</li>
</ul>
</li>
<li>다른 경우에는 멀티레이어가 나쁠 수도 있다.
<ul>
<li>예) 0~9숫자 인식, 그렇게 복잡하지 않은 데이터에 멀티레이어를 쓰면 vanishing gradient problem이 생길 수 있다.</li>
</ul>
</li>
</ul>
<h4 id="fatshort-vs-deeplong">Fat(Short) vs Deep(Long)</h4>
<figure>
    <img src="/images/pabii/neuralnetwork07.jpg"/> 
</figure>

<ul>
<li>딥러닝은 long model의 마케팅 용어</li>
</ul>
<p>ㅡㅡㅡ
하나의 레이어에 뉴런을 계속 추가하는 모델 : Fat model(short model)</p>
<p>레이어를 계속 추가하는 모델 : Long model(요즘에는 딥러닝)</p>
<p>short 모델은 함수를 찾아내는 작업이기 때문에 뉴런을 추가하면 할수록 피팅이 좋은데, overfitting의 문제가 있어서 pruning을 하거나 weight를 조금 줄이는 방식이 도움이 된다.</p>
<p>반면, long 모델은 vanishing 문제가 있고, 어떠한 경우에는 exploding 문제가 발생할 수 있기 때문에 모델 내재적으로 굉장히 약하다는 평가를 받습니다.</p>
<h2 id="svm-vs-deeplearning">SVM vs. DeepLearning</h2>
<h3 id="neural-network-모델의-기본적인-계산-구조">Neural Network 모델의 기본적인 계산 구조</h3>
<p>Linear Regression에서 Logistic Regression으로 넘어갈때만해도 error를 최소화하는 것에 관심을 가졌는데, SVM으로 넘어오면서 더 이상 error에 관심을 갖지 않고, 특정한 component들의 조건식을 만족시켜주는 theta값들의 조합을 찾아내는 작업으로 변했다.</p>
<ul>
<li>함수를 아는 케이스(SVM) vs. 함수를 모르고 그래표로 표현 해놓은 케이트(NN)</li>
</ul>
<h4 id="svm---직접-고른-kernel">SVM - 직접 고른 Kernel</h4>
<ul>
<li>함수를 가정하고 시작하는 방법</li>
</ul>
<p>SVM은 parametric으로 함수의 형태가 가정으로 정해져있는 방식이고, kernel(노말, 다항식, sigmoid)을 활용하여 input space(평면)에서 feature space(3차원 공간)으로 매핑을 해준 다음에 하나의 평면(hyperplane)을 찾아내는 작업이었습니다.</p>
<h4 id="deep-learning---neural-net이-고른-kernel">Deep learning - Neural net이 고른 Kernel</h4>
<ul>
<li>그래프 모델을 이루고 있는 함수를 찾아낼 수 있다. 단, 이론적으로 일대일대응이 되어야</li>
<li>NN를 블랙박스라는 표현을 하는데, 틀렸다. 왜냐면 함수 찾아낼 수 있으니까
<ul>
<li>\( x_1 \sim x_k \)까지 있기 때문에 functional shape을 찾을 수 있다.</li>
</ul>
</li>
</ul>
<p>ㅡㅡㅡ</p>
<ul>
<li>함수를 모르고 시작하는 방법</li>
</ul>
<p>뉴럴네트워크는 특정한 kernel이 없습니다. 우리가 지정해준 (4, 2, 2) 커널(네트워크의 모양)을 믿고 들어가는 것이다. 임의로 고른 kernel을 사용한다.</p>
<h5 id="결론">결론</h5>
<p>수학적으로 정의를 넣고 조정을 할 수 있는 모델이 SVM.<br>
수학적인 정의 없이 직관에 의지하고, 컴퓨터 계산에 의존하는 모델이 Neural Network. 뉴럴네트워크를 사용하면 어떤 일이 일어나는지 우리가 조정을 하지 못한다.</p>
<blockquote>
<p>모델을 만든다는 이야기는 &ldquo;내가 어떤 데이터를 쓰고, 함수의 형태가 어떤식으로 구성되어있기 때문에 결과값이 어떻게 된다.&rdquo; 는 식으로 입력과 출력사이에 어떤 작업이 일어나느지 우리가 알고 관리를 할수있어야하는데,<br>
뉴럴네트워크는 파라미터 값들은 다 계산을 했으니 볼수는 있다. 그런데 우리가 컨트롤 할 수 있는 구조가 아니다. 예) &ldquo;패러미터 하나를 빼버렸다.&rdquo; 그러면 다른 값들이 영향을 안받나? 모든 값들이 영향을 받는다. 그래서 제가 어떤 결과값들이 나오기 위해서 어떤 영향을 줬다고 하면, 결과값이 어떻게 바뀌는지 보고싶을때 컨트롤 할 수 있는 부분이 굉장히 제약이 있다.<br>
무조건 &ldquo;계산이 내가 손을 안대도 척척나오니 좋은거야.&rdquo; 라기보다는 내가 지금 이 계산에 손을 댈 수가없는 아주 사이언티픽하지 않은 모델임을 인지해주어야한다.<br>
통계학 교육 받은 입장에서는 굉장히 과학적이지 않은 모델이다. SVM은 어떻게 생각하면 가장 멀리 양보할수있는 지점이라 생각합니다</p>
</blockquote>
<h2 id="수학적인-구조">수학적인 구조</h2>
<h3 id="neural--network-모델의-기본적인-계산-구조">Neural  Network 모델의 기본적인 계산 구조</h3>
<h4 id="neural-net-모델의-수학적인-표현">Neural Net 모델의 수학적인 표현</h4>
<p>$$
\mathbb{P}(y=1|x)=f(x:\theta,W)=\sigma(\theta^T \sigma(Wx))
$$</p>
<p>$$
\mathcal{L}(\theta, W) =
$$</p>
<p>\( \mathcal{L}(\theta, W) \) : cross entropy 함수<br>
\(f(x:\theta,W)\) : sigmoid를 2번 사용</p>
<ul>
<li>첫번째는 \(Wx\)</li>
<li>두번째는 \( \theta \cdot \) 첫번째 식</li>
</ul>
<p>gradient descent처럼 미분값을 이용한다.</p>
<h4 id="neural-net-모델의-network-표현-및-계산-구조">Neural net 모델의 Network 표현 및 계산 구조</h4>
<p>Chain rule을 사용해서 미분<br>
gradient descent처럼 미분값을 이용한다.(+j값이 움직이는 것에 따라 각각 미분값이 생성된다, Jacobian)</p>
<h2 id="vanishing-gradient-problem--drop-out">Vanishing Gradient Problem &amp; Drop-out</h2>
<h3 id="neural-network-모델의-한계를-넘어서---training-부분">Neural Network 모델의 한계를 넘어서 - Training 부분</h3>
<h4 id="50-씩의-drop-out을-했을-때-모델의-변화">50% 씩의 Drop-out을 했을 때 모델의 변화</h4>
<p>vanishing gradient problem을 해결하는 방법</p>
<ul>
<li>pruning 작업을 통해서 계산 문제를 해결한다</li>
</ul>
<p>ㅡㅡㅡ</p>
<p>vanishing 문제 해결 : pruning 작업을 해야하는데 어떻게 해야할까? 모델 자체가 체계적인 프루닝이 한계가 있다.</p>
<ul>
<li>단순한 drop-out 작업을 한다(% 이용).</li>
</ul>
<p>각각의 레이어를 50%씩 없애 버린다. 남아있는 가중치는 더 영향이 강해질 것이다.</p>
<ul>
<li>입력값을 줄여서 어떤 특정 노드, path가 갖는 파워를 극대화해서 찾아보는 작업.(<strong>랜덤포레스트</strong>할때 데이터셋을 여러개로 쪼갠다음에 각각에서 나온 서브모델을 다 결합하는데 자주 등장하는 노드에는 높은 가중치, 아니면 낮은 가중치)</li>
<li>어떤 노드가 큰값을 가져가고, 어떤건 작은값을 가져간다. 이런 케이스를 여러 개 결합하면 당연히 큰값은 가중치 크게 들어가는게 구조화 될것이다. (<strong>앙상블</strong> 스트럭쳐 이용)</li>
</ul>
<h4 id="50씩-drop-out을-했을-때-모델-구성-방식">50%씩 Drop-out을 했을 때 모델 구성 방식</h4>
<ul>
<li>Drop-out : 랜덤워크에서 하는 방식</li>
</ul>
<p>ㅡㅡㅡ</p>
<p>앙상블의 아이디어를 이용해서 데이터셋을 4개로 쪼개 각각의 Network를 만들어낸다(50% drop-out). 4개의 셋에 같은 가중치를 두고 결합을 하게 되면 극대화된 파워들이 점점더 크게 인식될 것이다(가중치도 50%만큼).</p>
<h3 id="neural-network-모델의-한계를-넘어서---test-부분">Neural Network 모델의 한계를 넘어서 - Test 부분</h3>
<ul>
<li>노드 내에서는 overconfidence 문제가 생길 수도 있다.</li>
<li>모델이 널뛰기를 하고 있다는 느낌이 들면 y절편을 확인해본다.</li>
</ul>
<p>ㅡㅡㅡ</p>
<p>원래 가중치가 1이었는데 drop-out후 3이 나왔으면 -&gt; 1.5로 마지막에 들어간다.<br>
원래 가중치보다 크기때문에 해당 노드는 중요하고, 가중치를 많이 받아야하는 노드<br>
가중치를 이용한 weighted average방식을 이용해서 모델을 결합하는 부분 : NN의 pruning 기법(vanishing 문제를 해결해주는)</p>
<blockquote>
<p>우리가 뉴럴네트워크를 어떤 방식으로 썼습니까? <strong>DT</strong>를 이용하는 방식, DT는 단순한 직선 여기는 대각선같은 것을 쓰는, 즉 <strong>로지스틱 리그레션</strong> 같은 작업을 썼다. 데이터를 피딩해서 파라미터 업데이트방식이 <strong>부스팅</strong>계열의 방식, 마지막으로 배니싱 문제 해결하기 위해 프루닝 하는 작업이 <strong>랜덤 포레스트</strong> 작업.<br>
우리가 알고있는 뉴럴네트워크 모델은 시작부터 모델 빌딩, 파라미터 업데이트, 최종 레귤러라이제이션 작업까지, 즉 옵티마이제이션, 제네럴라이제이션 작업까지 이 모든 작업이 앞에서 배웠던 모델들의 결합이에요. 하늘에서 뚝 떨어진 모델아니고,  이때까지 앞에서 배운 모델을 다 배우고 있어야지 이 모델이 어떻게 돌아가는지 인지를 할 수 있는겁니다.<br>
결합 가능한 구조가 거의 무한대에 가깝게 나올거에요. 그러므로 drop-out해서 나오는 서브샘플 케이스가 많아질 수록 내제화된 구조로 오버피팅문제를 해결할 수 있다.(랜덤포레스트 처럼), 수학적으로 엄밀하게 정확한 방법은 아니지만 네트워크 모델의 특성상 프루닝 하기 힘든 상황을 인지했을때 이것보다 괜찮은 작업을 찾기 힘들기 때문에 best practice가 되어 많은 사람들이 쓰고 있다.</p>
</blockquote>
<h2 id="factor-analysis--auto-encoder">Factor Analysis &amp; Auto-encoder</h2>
<h3 id="neural-network-모델의-근본-로직">Neural Network 모델의 근본 로직</h3>
<p>여전히 중간 hidden-layer에 뭘 써야하는지 잘 모르겠다. &ldquo;(4,2)보다 (7,2)가 더 non-linear하고, (4,2,3)이 더 복잡한 데이터에 쓰인다.&rdquo; 정도의 느낌은 있는데&hellip;</p>
<h4 id="factor-analysis">Factor Analysis</h4>
<ul>
<li>linear combination일 때, factor들도 normal 분포를 따르면 원래 데이터도 normal을 따른다. 반대로 원래 데이터도 노말이면 팩터들도 노말이다.
<ul>
<li>FA가 선형 결합이라 많이 사용했다.</li>
</ul>
</li>
<li>ICA : 2번, 3번해서 non-linear component를 찾을 수 있다면?, non-linear function을 찾아내는 작업
<ul>
<li>예) 뉴럴 네트워크</li>
</ul>
</li>
</ul>
<p>ㅡㅡㅡ
PCA는 전체 데이터를 다 활용하는 것이었고, FA는 공분산만 쓰는 거였다.</p>
<p>FA는 latent variable : &ldquo;시험 20개를 설명하는 실제 인간의 능력은 5개밖에 없을 것이다&rdquo; 라는 가정을 가지고 5개의 숨겨진 변수를 찾아내는 작업을 할때 사용한다.</p>
<p>FA의 큰 가정 : 입력값과 출력값이 노말정규분포라면은 반드시 이런 형태의 숨겨진 구조를 가지고 있을 것이다.</p>
<ul>
<li>예) 이런 시험 점수가 노말정규분포를 따르고, 숨겨진 능력도 노말을 따른다고 가정을 하면<br>
노말 정규분포 끼리의 합은 노말 정규분포, 노말 정규분포 2개를 결합을 하면 노말이 나오고, 3개를 linear combination하면 2개의 노말정규분포를 뽑아낼 수 있다.</li>
</ul>
<p>But
만약 선형결합이 아니라 비선형결합이었다면?<br>
정규분포가 아니라 포아송분포였다면?<br>
랜덤분포가 아니라 패턴이 있는 데이터였다면?</p>
<ul>
<li>이러한 경우, FA는 데이터 형태가 우리가 알고있는 형태를 넘어서면 더이상 쓸 수 없는 모델이 된다.</li>
</ul>
<ol>
<li>분포 함수가 정규 분포가 아니라면?</li>
</ol>
<ul>
<li>Non-gaussian Prior를 쓰는 Independent Component Analysis(ICA) 모델을 이용해서 계산하는 방법이 있다.</li>
</ul>
<ol start="2">
<li>아예 구조를 바꿔줘야 한다면? (선형 -&gt; 비선형, 함수의 구조를 아예 바꿔야 하는 경우)</li>
</ol>
<ul>
<li>애초에 가정한 함수의 구조가 없는 모델을 사용하자 -&gt; 뉴럴네트워크</li>
<li>계산이 위험할수있으니 조심해야되긴하는데, 이 정도 상황에 직면했으면 뉴럴 네트워크한번 써보자는 맥락. 그게바로 오토인코더</li>
</ul>
<h4 id="auto-encoder-bottleneck-model">Auto-encoder (Bottleneck model)</h4>
<p>input : 중간고사 시험점수, normal 분포를 따르지 않는다고 가정</p>
<ul>
<li>목표 : 합리적인 latent variable을 뽑은 다음에 기말고사 시험점수를 예측하고 싶다.</li>
<li>중간에 Neural Network 계산 스템을 넣고, latent component를 찾아낸다고 해보자.
<ul>
<li>어떠한 함수를 잘 선택하면 변수를 잘 찾을 수 있다.</li>
<li>노말이 아니기 때문에 단순한 계산법을 사용할 수 없다.</li>
</ul>
</li>
<li>3개 변수를 잘 찾아서 기말고사 시험점수 예측하기
<ul>
<li>Encoder, Decoder를 어떻게 할 것인가? : 어렵다, 통계학에서 쳐주지도 않을듯</li>
</ul>
</li>
<li>PCA관점에서 20차원을 2~3차원으로 매핑해서 볼수있다는 관점</li>
</ul>
<p>ㅡㅡㅡ
예) 중간고사 20개 점수 입력, 기말고사 점수 예측(출력)하고 싶다.<br>
가운데 바틀넥이 5개의 중간변수 집어넣고.  input, latent가 노말이었으면 한번에 점프할수있었을텐데, 정규분포가 아니다.</p>
<ul>
<li>뭔가 복잡한 네트워크가 들어가야겠죠?</li>
<li>네트워크 이용해서 5개 변수 찾아낸다는건 결국 복잡한 비선형함수를 활용해서 5개 변수를 찾겠다와 똑같은 말이다.</li>
<li>FA의 일반형 버전을 찾아내기 위해서 뉴럴네트워크를 활용할수있는데 처음에 5개변수를 찾아내는 용으로 인코더, 5개 이용해서 기말고사 점수 찾아내는 작업을 갖다가 디코더
<ul>
<li>가운데는 딱 제한된 숫자만 들어가는 FA의 구조를 바틀넥, 전체를 오토인코더</li>
</ul>
</li>
</ul>
<p>히든레이어를 4칸을 쓰겠습니다. 하나하나 안에 몇개의 컴포넌트를 쓰겠습니다. 이야기하는건 특정 레이어가 변수 3개 만으로 공간을 재구성해줄 것이다라는 식으로 접근할 수 있는 거에요. 레이어 몇개쓸래? 레이어 안에 변수 몇 개 쓸래?는 같은 맥락으로 한 레이어에 3개변수를 쓴다는 거는 그 특정한 스페이스 3차원의 공간안에 데이터를 다 매핑할 수 있다.라는 관점으로 생각.</p>
<h2 id="tensorflow의-tensor란-tensorflow가-돌아가는-구조는">TensorFlow의 Tensor란? TensorFlow가 돌아가는 구조는?</h2>
<p>텐서라는게 무엇인가? 왜 다들 그렇게 그래픽카드 연산을 이야기 하는가?</p>
<h3 id="tensor--multi-dimension-matrix">Tensor = Multi-dimension matrix</h3>
<p>텐서 = 행렬을 여러개 모아 놓은 값</p>
<p>Tensorflow : Multi-dimension Matrix를 계산해주는 함수들을 모아놓은 것</p>
<ol>
<li>
<p>CPU는 데이터 처리하는데 행렬데이터 이상의 복잡한 차원을 가진 데이터를 처리해준다.<br>
CPU는 페라리, 적재공간이 하나도 없다, 속도는 엄청 빠르다, CPU는 아주 작은 캐시메모리에 데이터 입력하고 아주 빠른 속도로 왔다갔다 처리</p>
</li>
<li>
<p>GPU는 덤프트럭, 적재공간은 많은데 속도는 느리다. 큰 데이터를 처리.<br>
예를들어, 3D게임, 색상이 다양한 종류의 게임을 한다면  속력과 방향 값이 들어간다면</p>
</li>
</ol>
<blockquote>
<p>X, Y, Z, R, G, B, 속력, 방향 -&gt; 총 8개의 component가 입력되어야하는데, 데이터가 포인트가 많으면 실제 움직이는 것과 유사할텐데, component가 적고, 데이터포인트가 적으면 굉장히 부자연스럽겠죠?<br>
즉, 그래픽이 화려하다 = 결국은 데이터가 많이 들어갔다. 대용량 데이터 처리해야하는데 tensor 계산이 절실하다. 그래픽카드가 이런 복잡한 행렬 계산용으로 처리가 되는 코어가 따로 구성되어있다. 코어의 일부를 텐서플로우가 활용할 수 있도록 CUDA라는 방식, CUDA 코어를 만들고, 텐서플로어가 활용된다.
(R코드 에시로 mxnet, 페이스북꺼, 쿠다코어가 아닌 다른 코어도 행렬계산이 가능하다. 텐서 활용안하고 자기가 직접 코드를 짜서 만들수도있습니다.) 파이썬, R, C언어, 자바에 텐서플로우 지원이 있는데, 진짜 실력자는 자기가 직접 코드를 짠다. C기반의 줄리아라는 언어가 있다. Julia를 꾀나 들어본 상황에서 공부를 하고 있지 않을까 생각합니다.</p>
</blockquote>
<h2 id="5회자-마무리">5회자 마무리</h2>
<p>머신러닝, 딥러닝, 인공지능 :</p>
<p>뉴럴네트워크는 단순히 모델들을 결합한거에 불과하지, 뭔가 3차원, 4차원 같은 모델이 아니다.</p>
<p>계산을 빨리해야하는 이유가,</p>
<ul>
<li>모델이 수학적으로 뭔가 정의를 하는게 아니라 컴퓨터가 알아서 해줘라는 방식의 단순한 떠맡기기 모델이기 때문에</li>
<li>모델 자체의 문제도 많이 있고, 그럼에도 불구하고 몇군데 쓰임새가 있기때문에</li>
<li>계산을 하기위해 컴퓨터 자원을 굉장히 많이 활용하고, 대부분 행렬계산, 텐서계산을 해야하기 때문에 그래픽카드를 많이 쓴다.</li>
<li>그래픽 카드는 이러한 구조의 계산에 특화되어있다.</li>
</ul>

  
    
    <div class="post-toc">
      <span class="title">Contents</span>
      <nav id="TableOfContents">
  <ul>
    <li><a href="#neural-network-모델의-아이디어">Neural network 모델의 아이디어</a>
      <ul>
        <li><a href="#deep-learning--인공지능">Deep Learning = 인공지능?</a></li>
        <li><a href="#딥러닝은-좀-더-무거운-패턴-인식-알고리즘일-뿐이다">딥러닝은 (좀 더 무거운) 패턴 인식 알고리즘일 뿐이다.</a></li>
      </ul>
    </li>
    <li><a href="#neural-network-모델-간단-예제">Neural network 모델 간단 예제</a>
      <ul>
        <li><a href="#왜-multi-layer-모델이-필요할까-왜-multi-layer-모델이-더-적합도를-높여줄-수-있을까">왜 Multi-layer 모델이 필요할까? 왜 Multi-layer 모델이 더 적합도를 높여줄 수 있을까?</a></li>
        <li><a href="#feed-forward--back-propagation-계산-방식---1">Feed Forward + Back propagation 계산 방식 - 1</a></li>
        <li><a href="#feed-forward--back-propagation-계산-방식---2">Feed Forward + Back propagation 계산 방식 - 2</a></li>
        <li><a href="#기준점-threshold가-여러개인-경우">기준점 (Threshold)가 여러개인 경우</a></li>
      </ul>
    </li>
    <li><a href="#network의-형태---1">Network의 형태 - 1</a>
      <ul>
        <li><a href="#네트워크가-구성되는-방식에-따라">네트워크가 구성되는 방식에 따라</a></li>
      </ul>
    </li>
    <li><a href="#network-theory-기본-개념">Network Theory 기본 개념</a>
      <ul>
        <li><a href="#네트워크-모델은-tree-모델의-사우이-개념---metric--network-distance-euclidean-distance-x">네트워크 모델은 Tree 모델의 사우이 개념 - Metric = Network distance (Euclidean distance X)</a></li>
      </ul>
    </li>
    <li><a href="#network-theory---직접-계산">Network Theory - 직접 계산</a>
      <ul>
        <li><a href="#matrix-형태로-변환-후-계산에-활용">Matrix 형태로 변환 후 계산에 활용</a></li>
      </ul>
    </li>
    <li><a href="#stability-vs-efficiency">Stability vs. Efficiency</a>
      <ul>
        <li><a href="#중앙집권적인-시스템은-중앙이-무너지면-끝--탈중앙집권적이면-효율성이-보장될까">중앙집권적인 시스템은 중앙이 무너지면 끝 &amp; 탈중앙집권적이면 효율성이 보장될까?</a></li>
      </ul>
    </li>
    <li><a href="#network의-형태---2">Network의 형태 - 2</a>
      <ul>
        <li><a href="#필요한-deepness에-따라">필요한 &ldquo;Deep&quot;ness에 따라</a></li>
      </ul>
    </li>
    <li><a href="#network의-형태---3">Network의 형태 - 3</a>
      <ul>
        <li><a href="#원하는-결과값의-형태에-따라">원하는 결과값의 형태에 따라</a></li>
        <li><a href="#답이--y_1y_2y_3-으로-나올-때">답이 \( Y_1,Y_2,Y_3 \)으로 나올 때</a></li>
      </ul>
    </li>
    <li><a href="#neural-network는-결국-logistic-regression을-여러-단계로-결합한-모델이다">Neural Network는 결국 Logistic regression을 여러 단계로 결합한 모델이다</a>
      <ul>
        <li><a href="#activation-function에-따라-logistic이-아닌-다른-regression의-결합이-될-수도-있음">Activation function에 따라 Logistic이 아닌 다른 regression의 결합이 될 수도 있음</a></li>
      </ul>
    </li>
    <li><a href="#신경망-모델을-정의하는-또-하나의-함수---activation-function">신경망 모델을 정의하는 또 하나의 함수 - Activation function</a>
      <ul>
        <li><a href="#activation-function은-따로-정해진-것이-아니라-목적에-맞게-바꾸어가며-이용할-수-있음">Activation function은 따로 정해진 것이 아니라, 목적에 맞게 바꾸어가며 이용할 수 있음</a></li>
      </ul>
    </li>
  </ul>

  <ul>
    <li><a href="#deep-network-vs-fat-network">Deep Network vs. Fat Network</a>
      <ul>
        <li><a href="#어떤-모델이-더-좋은-모델일까">어떤 모델이 더 좋은 모델일까?</a></li>
      </ul>
    </li>
    <li><a href="#svm-vs-deeplearning">SVM vs. DeepLearning</a>
      <ul>
        <li><a href="#neural-network-모델의-기본적인-계산-구조">Neural Network 모델의 기본적인 계산 구조</a></li>
      </ul>
    </li>
    <li><a href="#수학적인-구조">수학적인 구조</a>
      <ul>
        <li><a href="#neural--network-모델의-기본적인-계산-구조">Neural  Network 모델의 기본적인 계산 구조</a></li>
      </ul>
    </li>
    <li><a href="#vanishing-gradient-problem--drop-out">Vanishing Gradient Problem &amp; Drop-out</a>
      <ul>
        <li><a href="#neural-network-모델의-한계를-넘어서---training-부분">Neural Network 모델의 한계를 넘어서 - Training 부분</a></li>
        <li><a href="#neural-network-모델의-한계를-넘어서---test-부분">Neural Network 모델의 한계를 넘어서 - Test 부분</a></li>
      </ul>
    </li>
    <li><a href="#factor-analysis--auto-encoder">Factor Analysis &amp; Auto-encoder</a>
      <ul>
        <li><a href="#neural-network-모델의-근본-로직">Neural Network 모델의 근본 로직</a></li>
      </ul>
    </li>
    <li><a href="#tensorflow의-tensor란-tensorflow가-돌아가는-구조는">TensorFlow의 Tensor란? TensorFlow가 돌아가는 구조는?</a>
      <ul>
        <li><a href="#tensor--multi-dimension-matrix">Tensor = Multi-dimension matrix</a></li>
      </ul>
    </li>
    <li><a href="#5회자-마무리">5회자 마무리</a></li>
  </ul>
</nav>
    </div>
    <div style='padding:15px;'>
    </div>
    
  </section>
  <div class="post-meta-code">
    <div class="desc">
      
      <a href="mailto:uqpuark@gmail.com">uqpuark</a>
      
      님이
      <span class="highlight">2021년 03월 18일 21시 29분</span> 
      에 작성한 글입니다.
    </div>
    <div style="padding-left: 16px">
    데이터 사이언티스트를 꿈꾸고 있습니다. 수학, 통계학에 대한 질문이나 의견을 나누고 싶습니다. 
    </div>
    <div class="desc">
      
      <div class="desc">
        <span class="fixed-desc">[카테고리]</span>
        
        
      </div>
      
      <div class="desc">
        <span class="fixed-desc">[태그]</span>
        
      </div>
    </div>
  </div>  
  <div class="recommend-articles">
    다음으로 읽을만한 글입니다.
    <ul>
      
      <li>
        <a href="https://uqpuark.github.io/2021_01/pp_01/" rel="prev">
          <span>[생산계획] CH4. FORECASTING</span>
        </a>
      </li>
      
      
    </ul>
  </div>
</div>

<script src="https://utteranc.es/client.js"
        repo="uqpuark/uqpuark.github.io"
        issue-term="pathname"
        theme="github-light"
        crossorigin="anonymous"
        async>
</script>

<div class="go-top">
  <a href="#" class="go-top-button">
    <i class="fa fa-angle-double-up"></i>
    <span>위로</span>
  </a>
</div>
<footer class="footer">
  <div class="share">
      <a href="https://github.com" title="Github" target="_blank"><i class="fa fa-github fa-3x"></i></a>
  </div>
  COPYRIGHT (C) <a href="https://blog.lulab.net">DONGGEUN,BANG (LUBANG).</a><br />
  ALL RIGHTS RESERVED.
<script>
window.store = {
    
    
}
</script>

<script src="/js/lunr.min.js"></script>
<script src="/js/search.js"></script>
</footer>
</body>
</html>
